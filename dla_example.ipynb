{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Установка"
      ],
      "metadata": {
        "id": "u2oymZgy9ZlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahMF66mT4W_z",
        "outputId": "188035bf-e9f2-4e9c-d63f-6bc58bb93505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asr_project_1'...\n",
            "remote: Enumerating objects: 403, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 403 (delta 228), reused 383 (delta 208), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (403/403), 803.30 KiB | 23.63 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/zakoden/asr_project_1 -b hw_asr_2022"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./asr_project_1\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeECiVmtLOts",
        "outputId": "e1d32a19-def6-4305-f1cc-69e9fd729675"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/asr_project_1\n",
            "calc_metrics.py  \u001b[0m\u001b[01;34mhw_asr\u001b[0m/  README.md         \u001b[01;34mtest_data\u001b[0m/  train.py\n",
            "Dockerfile       LICENSE  requirements.txt  test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD5fttGsNAAy",
        "outputId": "5fb3cd45-f10c-4a24-fd72-28c5aebe9478"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 11 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n",
            "Collecting speechbrain~=0.5.12\n",
            "  Downloading speechbrain-0.5.13-py3-none-any.whl (498 kB)\n",
            "\u001b[K     |████████████████████████████████| 498 kB 67.0 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 72.8 MB/s \n",
            "\u001b[?25hCollecting torch_audiomentations\n",
            "  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.5.3)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 44.5 MB/s \n",
            "\u001b[?25hCollecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.4.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting torchaudio~=0.11.0\n",
            "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting hyperpyyaml\n",
            "  Downloading HyperPyYAML-1.0.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.12->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.12->-r requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from speechbrain~=0.5.12->-r requirements.txt (line 9)) (21.3)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.49.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 2)) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2022.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 10)) (2022.8.2)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 10)) (0.3.5.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 10)) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 10)) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 10)) (3.8.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain~=0.5.12->-r requirements.txt (line 9)) (3.8.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting torch-pitch-shift>=1.2.2\n",
            "  Downloading torch_pitch_shift-1.2.2-py3-none-any.whl (5.0 kB)\n",
            "Collecting julius<0.3,>=0.2.3\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations->-r requirements.txt (line 11)) (0.8.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (0.56.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (0.11.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (1.0.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (1.6.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations->-r requirements.txt (line 11)) (2.21)\n",
            "Collecting primePy>=1.3\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 13)) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 13)) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 13)) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 76.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 77.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 79.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 69.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 67.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 76.0 MB/s \n",
            "\u001b[?25hCollecting hypothesis<7,>=6.14\n",
            "  Downloading hypothesis-6.56.3-py3-none-any.whl (395 kB)\n",
            "\u001b[K     |████████████████████████████████| 395 kB 73.3 MB/s \n",
            "\u001b[?25hCollecting pygtrie<3.0,>=2.1\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis<7,>=6.14->pyctcdecode->-r requirements.txt (line 14)) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Collecting ruamel.yaml>=0.17.8\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 70.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: julius, hyperpyyaml, pathtools\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21894 sha256=588a1a681a8a53a998dd07b70d2f68c786adc4e62fdaca227412427915dc179c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/52/2c/7dd069f82c7f905f40b190a8039ec2a17fdd4bb009c57c6664\n",
            "  Building wheel for hyperpyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyperpyyaml: filename=HyperPyYAML-1.0.1-py3-none-any.whl size=15192 sha256=440ee4ac73d5584535e645d9f69d7a60a26ffb6d1e3d5770e33a17ea7058d5b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/87/65/266d722c3932f81f16332ce842e972be8421e3a9cd3771766b\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=ff9472e186f30214561f708bb823413cce1b09d738289b2de2a5d2f5740fccef\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built julius hyperpyyaml pathtools\n",
            "Installing collected packages: urllib3, torch, smmap, ruamel.yaml.clib, torchaudio, ruamel.yaml, primePy, gitdb, exceptiongroup, xxhash, torch-pitch-shift, shortuuid, setproctitle, sentry-sdk, sentencepiece, responses, pygtrie, pathtools, multiprocess, julius, hypothesis, hyperpyyaml, huggingface-hub, GitPython, docker-pycreds, wandb, torchvision, torch-audiomentations, speechbrain, pyctcdecode, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.1+cu113\n",
            "    Uninstalling torchaudio-0.12.1+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 datasets-2.6.1 docker-pycreds-0.4.0 exceptiongroup-1.0.0rc9 gitdb-4.0.9 huggingface-hub-0.10.1 hyperpyyaml-1.0.1 hypothesis-6.56.3 julius-0.2.7 multiprocess-0.70.13 pathtools-0.1.2 primePy-1.3 pyctcdecode-0.4.0 pygtrie-2.5.0 responses-0.18.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 sentencepiece-0.1.97 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 speechbrain-0.5.13 torch-1.11.0 torch-audiomentations-0.11.0 torch-pitch-shift-1.2.2 torchaudio-0.11.0 torchvision-0.12.0 urllib3-1.25.11 wandb-0.13.4 xxhash-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка весов модели"
      ],
      "metadata": {
        "id": "k8hSDXbECXsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir default_test_model"
      ],
      "metadata": {
        "id": "mEKb7LCeIJAE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "веса"
      ],
      "metadata": {
        "id": "0Zn0OqU2Glw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/189DH4ir-EPmxOa6LIjN2AjheO-DBA56M/view?usp=sharing -O default_test_model/checkpoint.pth"
      ],
      "metadata": {
        "id": "_xBHsqq9dl0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06054f45-25b4-47ab-80da-36a109697a2c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=189DH4ir-EPmxOa6LIjN2AjheO-DBA56M\n",
            "To: /content/asr_project_1/default_test_model/checkpoint.pth\n",
            "\r  0% 0.00/17.9M [00:00<?, ?B/s]\r100% 17.9M/17.9M [00:00<00:00, 233MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "конфиг"
      ],
      "metadata": {
        "id": "ybXoZPWuGoKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1xJL65pw8DDweZwertcMZnowbnK9bkPIR/view?usp=sharing -O default_test_model/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsBlgIxGGfId",
        "outputId": "aa1fcab2-a022-4dfb-f120-44551b28aedb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xJL65pw8DDweZwertcMZnowbnK9bkPIR\n",
            "To: /content/asr_project_1/default_test_model/config.json\n",
            "\r  0% 0.00/3.52k [00:00<?, ?B/s]\r100% 3.52k/3.52k [00:00<00:00, 6.26MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Валидация модели"
      ],
      "metadata": {
        "id": "asY2CgiaAmxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 test.py -c hw_asr/configs/config_test_clean.json -r default_test_model/checkpoint.pth -o test_result.json"
      ],
      "metadata": {
        "id": "bi7DK4HldlxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d77422-2058-4df6-adf2-2eb908d43deb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSpeechModel(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (rnn_block): GRU(1024, 80, num_layers=7, batch_first=True, bidirectional=True)\n",
            "  (seq_batch_norm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 1483836\n",
            "Loading checkpoint: default_test_model/checkpoint.pth ...\n",
            "  0% 0/131 [00:07<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"test.py\", line 174, in <module>\n",
            "    main(config, args.output)\n",
            "  File \"test.py\", line 71, in main\n",
            "    batch[\"probs\"][i], batch[\"log_probs_length\"][i], beam_size=10\n",
            "  File \"/content/asr_project_1/hw_asr/text_encoder/ctc_char_text_encoder.py\", line 71, in ctc_beam_search\n",
            "    hypos_dict = dict(list(sorted(new_hypos_dict.items(), key=lambda x: x[1]))[-beam_size:])\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подсчёт метрик"
      ],
      "metadata": {
        "id": "-zZNb0Tt-dg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test-clean"
      ],
      "metadata": {
        "id": "QlPArmXrW43E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 calc_metrics.py -c hw_asr/configs/config_test_clean.json -r default_test_model/checkpoint.pth -o test_result.json"
      ],
      "metadata": {
        "id": "KchrmE5RdluP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d07d6b-9685-48be-e3b5-f0d4ef6181a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSpeechModel(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (rnn_block): GRU(1024, 80, num_layers=7, batch_first=True, bidirectional=True)\n",
            "  (seq_batch_norm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 1483836\n",
            "Loading checkpoint: default_test_model/checkpoint.pth ...\n",
            "100% 131/131 [25:57<00:00, 11.89s/it]\n",
            "argmax_CER: 0.12841983660501496\n",
            "beam_search_CER: 0.12649989923354563\n",
            "argmax_WER: 0.39375138396538767\n",
            "beam_search_WER: 0.3868371454231323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test-other"
      ],
      "metadata": {
        "id": "ypDX9r4nW6cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 calc_metrics.py -c hw_asr/configs/config_test_other.json -r default_test_model/checkpoint.pth -o test_result.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9OLfhYoWsdu",
        "outputId": "abcbc775-d63a-450e-d26e-061db7a93c32"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading part test-other\n",
            "Downloading https://www.openslr.org/resources/12/test-other.tar.gz to /content/asr_project_1/data/datasets/librispeech/test-other.tar.gz\n",
            "test-other.tar.gz: 329MB [00:02, 158MB/s]               \n",
            "Preparing librispeech folders: test-other: 100% 90/90 [00:00<00:00, 185.54it/s]\n",
            "DeepSpeechModel(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (rnn_block): GRU(1024, 80, num_layers=7, batch_first=True, bidirectional=True)\n",
            "  (seq_batch_norm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 1483836\n",
            "Loading checkpoint: default_test_model/checkpoint.pth ...\n",
            "100% 147/147 [24:39<00:00, 10.06s/it]\n",
            "argmax_CER: 0.28845883994279464\n",
            "beam_search_CER: 0.28934319260795294\n",
            "argmax_WER: 0.6596116064091321\n",
            "beam_search_WER: 0.6575636405840986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Запуск обучения"
      ],
      "metadata": {
        "id": "4ihzowmy9Ryi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -c hw_asr/configs/config_train_small.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iwnCAwING5h",
        "outputId": "8d9ca555-b667-4c6c-8b5a-7d7ad47ff656"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading part train-clean-100\n",
            "Downloading https://www.openslr.org/resources/12/train-clean-100.tar.gz to /content/asr_project_1/data/datasets/librispeech/train-clean-100.tar.gz\n",
            "train-clean-100.tar.gz: 6.39GB [05:23, 19.7MB/s]                \n",
            "Preparing librispeech folders: train-clean-100: 100% 585/585 [00:35<00:00, 16.51it/s]\n",
            "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
            "Filtered 13243(46.4%) records  from dataset\n",
            "Loading part dev-clean\n",
            "Downloading https://www.openslr.org/resources/12/dev-clean.tar.gz to /content/asr_project_1/data/datasets/librispeech/dev-clean.tar.gz\n",
            "dev-clean.tar.gz: 338MB [00:20, 16.6MB/s]               \n",
            "Preparing librispeech folders: dev-clean: 100% 97/97 [00:00<00:00, 155.86it/s]\n",
            "Loading part test-other\n",
            "Downloading https://www.openslr.org/resources/12/test-other.tar.gz to /content/asr_project_1/data/datasets/librispeech/test-other.tar.gz\n",
            "test-other.tar.gz: 329MB [00:19, 16.5MB/s]               \n",
            "Preparing librispeech folders: test-other: 100% 90/90 [00:00<00:00, 131.66it/s]\n",
            "Loading part test-clean\n",
            "Downloading https://www.openslr.org/resources/12/test-clean.tar.gz to /content/asr_project_1/data/datasets/librispeech/test-clean.tar.gz\n",
            "test-clean.tar.gz: 347MB [00:20, 17.0MB/s]               \n",
            "Preparing librispeech folders: test-clean: 100% 87/87 [00:00<00:00, 144.59it/s]\n",
            "DeepSpeechModel(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (rnn_block): GRU(1024, 64, num_layers=7, batch_first=True, bidirectional=True)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 1120316\n",
            "Warning: There's no GPU available on this machine,training will be performed on CPU.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "train:   0% 0/2000 [00:00<?, ?it/s]Train Epoch: 1 [0/2000 (0%)] Loss: 7.650401\n",
            "train:   0% 1/2000 [01:30<50:24:47, 90.79s/it]\n",
            "Saving model on keyboard interrupt\n",
            "Saving checkpoint: saved/models/default_config/1017_172151/checkpoint-epoch1.pth ...\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 107, in <module>\n",
            "    main(config)\n",
            "  File \"train.py\", line 71, in main\n",
            "    trainer.train()\n",
            "  File \"/content/asr_project_1/hw_asr/base/base_trainer.py\", line 73, in train\n",
            "    raise e\n",
            "  File \"/content/asr_project_1/hw_asr/base/base_trainer.py\", line 69, in train\n",
            "    self._train_process()\n",
            "  File \"/content/asr_project_1/hw_asr/base/base_trainer.py\", line 82, in _train_process\n",
            "    result = self._train_epoch(epoch)\n",
            "  File \"/content/asr_project_1/hw_asr/trainer/trainer.py\", line 94, in _train_epoch\n",
            "    metrics=self.train_metrics,\n",
            "  File \"/content/asr_project_1/hw_asr/trainer/trainer.py\", line 150, in process_batch\n",
            "    batch[\"loss\"].backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 363, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  CER (argmax)_train ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  WER (argmax)_train ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch_ ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     grad norm_train ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning rate_train ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_train ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  CER (argmax)_train 0.90082\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  WER (argmax)_train 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch_ 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     grad norm_train 10.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: learning rate_train 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_train 7.6504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/asr_project_1/wandb/offline-run-20221017_173102-hmo1w0kg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20221017_173102-hmo1w0kg/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7bhaqPmSk6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}